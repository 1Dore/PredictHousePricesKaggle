selected_features <- setdiff(selected_features, "median_house_value")
test_selected <- test_processed[, selected_features]
# Asegurarse de que test_selected no contenga factores que no estén en el train
for (col in selected_features) {
if (is.factor(test_selected[[col]])) {
test_selected[[col]] <- factor(test_selected[[col]], levels = levels(train_selected[[col]]))
}
}
# Predicción en el conjunto de prueba
test_predictions <- predict(best_model, newdata = test_selected)
# Generar el archivo de salida
submission <- data.frame(id = test_processed$id, median_house_value = test_predictions)
write.csv(submission, "submission.csv", row.names = FALSE)
best_model
# Ridge Regression
set.seed(123)
ridge_model <- train(median_house_value ~ ., data = train_selected, method = "ridge", trControl = train_control,
tuneLength = 10)
print(ridge_model)
# Lasso Regression
set.seed(123)
lasso_model <- train(median_house_value ~ ., data = train_selected, method = "lasso", trControl = train_control,
tuneLength = 10)
print(lasso_model)
# Comparar RMSE de los modelos, incluyendo Ridge y Lasso
results <- resamples(list(rf = rf_model, xgb = xgb_model, ridge = ridge_model, lasso = lasso_model))
summary(results)
# Hyperparameter tuning for XGBoost with more extensive grid search
xgb_grid_2 <- expand.grid(
nrounds = c(100, 200),
max_depth = c(6, 10),
eta = c(0.01, 0.1, 0.3),
gamma = c(0, 0.1, 0.2),
colsample_bytree = c(0.8, 1),
min_child_weight = c(1, 3),
subsample = c(0.8, 1)
)
# Custom train function for xgbTree to avoid the warning
xgb_custom <- trainControl(method = "cv", number = 5)
# Define the train function with additional control to avoid the warning
xgb_model_2 <- train(median_house_value ~ ., data = train_selected, method = "xgbTree",
trControl = xgb_custom, tuneGrid = xgb_grid_2,
verbose = FALSE, nthread = 1)
# Hyperparameter tuning for XGBoost with more extensive grid search
xgb_grid_2 <- expand.grid(
nrounds = c(100, 200),
max_depth = c(6, 10),
eta = c(0.01, 0.1, 0.3),
gamma = c(0, 0.1, 0.2),
colsample_bytree = c(0.8, 1),
min_child_weight = c(1, 3),
subsample = c(0.8, 1)
)
# Custom train function for xgbTree to avoid the warning
xgb_custom <- trainControl(method = "cv", number = 5)
# Define the train function with additional control to avoid the warning
xgb_model_2 <- train(median_house_value ~ ., data = train_selected, method = "xgbTree",
trControl = xgb_custom, tuneGrid = xgb_grid_2,
verbose = FALSE, nthread = 1)
print(xgb_model_2)
best_model <- if(min(rf_model$results$RMSE) < min(xgb_model_2$results$RMSE) &
min(rf_model$results$RMSE) < min(ridge_model$results$RMSE) &
min(rf_model$results$RMSE) < min(lasso_model$results$RMSE)) {
rf_model
} else if (min(xgb_model_2$results$RMSE) < min(ridge_model$results$RMSE) &
min(xgb_model_2$results$RMSE) < min(lasso_model$results$RMSE)) {
xgb_model_2
} else if (min(ridge_model$results$RMSE) < min(lasso_model$results$RMSE)) {
ridge_model
} else {
lasso_model
}
selected_features <- setdiff(selected_features, "median_house_value")
test_selected <- test_processed[, selected_features]
for (col in selected_features) {
if (is.factor(test_selected[[col]])) {
test_selected[[col]] <- factor(test_selected[[col]], levels = levels(train_selected[[col]]))
}
}
test_predictions <- predict(best_model, newdata = test_selected)
submission <- data.frame(id = test_processed$id, median_house_value = test_predictions)
write.csv(submission, "submission.csv", row.names = FALSE)
best_model
best_model <- if(min(rf_model$results$RMSE) < min(xgb_model_2$results$RMSE) &
min(rf_model$results$RMSE) < min(ridge_model$results$RMSE) &
min(rf_model$results$RMSE) < min(lasso_model$results$RMSE)) {
rf_model
} else if (min(xgb_model_2$results$RMSE) < min(ridge_model$results$RMSE) &
min(xgb_model_2$results$RMSE) < min(lasso_model$results$RMSE)) {
xgb_model_2
} else if (min(ridge_model$results$RMSE) < min(lasso_model$results$RMSE)) {
ridge_model
} else {
lasso_model
}
selected_features <- setdiff(selected_features, "median_house_value")
test_selected <- test_processed[, selected_features]
for (col in selected_features) {
if (is.factor(test_selected[[col]])) {
test_selected[[col]] <- factor(test_selected[[col]], levels = levels(train_selected[[col]]))
}
}
test_predictions <- predict(best_model, newdata = test_selected)
submission <- data.frame(id = test_processed$id, median_house_value = test_predictions)
write.csv(submission, "submission_esperanza.csv", row.names = FALSE)
# Cargar los datasets
train <- read.csv("train.csv")
test <- read.csv("test.csv")
# Seleccionar solo las características seleccionadas para el entrenamiento
train$ocean_proximity_num <- as.numeric(as.factor(train$ocean_proximity))
test$ocean_proximity_num <- as.numeric(as.factor(test$ocean_proximity))
num_clusters <- 5
clusters <- kmeans(train[, c("longitude", "latitude", "ocean_proximity_num")], centers = num_clusters)
train$position_cluster <- as.factor(clusters$cluster)
clusters <- kmeans(test[, c("longitude", "latitude", "ocean_proximity_num")], centers = num_clusters)
test$position_cluster <- as.factor(clusters$cluster)
# Imputar valores nulos en total_bedrooms
train$total_bedrooms[is.na(train$total_bedrooms)] <- median(train$total_bedrooms, na.rm = TRUE)
test$total_bedrooms[is.na(test$total_bedrooms)] <- median(test$total_bedrooms, na.rm = TRUE)
# Codificar variables categóricas
train$ocean_proximity <- as.factor(train$ocean_proximity)
test$ocean_proximity <- as.factor(test$ocean_proximity)
# Escalar características numéricas
num_features <- names(train)[sapply(train, is.numeric) & names(train) != "median_house_value" & names(train) != "id"]
preProcValues <- preProcess(train[, num_features], method = c("center", "scale"))
train_processed <- train
train_processed[, num_features] <- predict(preProcValues, train[, num_features])
test_processed <- test
test_processed[, num_features] <- predict(preProcValues, test[, num_features])
# Guardar los nuevos datasets procesados
# write.csv(train_processed, "train_processed.csv", row.names = FALSE)
# write.csv(test_processed, "test_processed.csv", row.names = FALSE)
continuas =  names(train)[sapply(train, is.numeric) & names(train) != "median_house_value" & names(train) != "id"]
continuas
for (column_name  in  continuas) {
vector_numerico = unlist(train_processed[column_name])
Q1 <- quantile(vector_numerico, 0.25)  # Primer cuartil (Q1)
Q3 <- quantile(vector_numerico, 0.75)  # Tercer cuartil (Q3)
IQR <- Q3 - Q1  # Rango intercuartílico
# Definir los límites inferior y superior para identificar outliers
limite_inferior <- Q1 - 1.5 * IQR
limite_superior <- Q3 + 1.5 * IQR
# Identificar los outliers
outliers <- vector_numerico[vector_numerico < limite_inferior | vector_numerico > limite_superior]
# Contar los outliers
conteo_outliers <- length(outliers)
# Resultados
print(column_name)
print(paste("Número de outliers identificados usando IQR:", conteo_outliers))
print(paste("Prosnetaje de outliers identificados usando IQR:", conteo_outliers/nrow(train)))
}
chart.Correlation(train[continuas], histogram=TRUE)
remove_outliers <- function(df, columns) {
for (col in columns) {
Q1 <- quantile(df[[col]], 0.25)
Q3 <- quantile(df[[col]], 0.75)
IQR <- Q3 - Q1
df <- df[!(df[[col]] < (Q1 - 1.5 * IQR) | df[[col]] > (Q3 + 1.5 * IQR)), ]
}
return(df)
}
numeric_columns_train <- sapply(train_processed, is.numeric)
train_processed_no_outliers <- remove_outliers(train_processed, names(numeric_columns_train[numeric_columns_train]))
numeric_columns_test <- sapply(test_processed, is.numeric)
test_processed_no_outliers <- remove_outliers(test_processed, names(numeric_columns_test[numeric_columns_test]))
cat("Tamaño del dataset train_processed original: ", nrow(train_processed), "\n")
cat("Tamaño del dataset train_processed sin outliers: ", nrow(train_processed_no_outliers), "\n")
cat("Tamaño del dataset test_processed original: ", nrow(test_processed), "\n")
cat("Tamaño del dataset test_processed sin outliers: ", nrow(test_processed_no_outliers), "\n")
for (column_name  in  continuas) {
vector_numerico = unlist(train_processed_no_outliers[column_name])
Q1 <- quantile(vector_numerico, 0.25)  # Primer cuartil (Q1)
Q3 <- quantile(vector_numerico, 0.75)  # Tercer cuartil (Q3)
IQR <- Q3 - Q1  # Rango intercuartílico
# Definir los límites inferior y superior para identificar outliers
limite_inferior <- Q1 - 1.5 * IQR
limite_superior <- Q3 + 1.5 * IQR
# Identificar los outliers
outliers <- vector_numerico[vector_numerico < limite_inferior | vector_numerico > limite_superior]
# Contar los outliers
conteo_outliers <- length(outliers)
# Resultados
print(column_name)
print(paste("Número de outliers identificados usando IQR:", conteo_outliers))
print(paste("Prosnetaje de outliers identificados usando IQR:", conteo_outliers/nrow(train)))
}
chart.Correlation(train_processed_no_outliers[continuas], histogram=TRUE)
selected_features
# Seleccionar solo las características seleccionadas para el entrenamiento
train_selected <- train_processed_no_outliers[, c(selected_features, "median_house_value")]
test_selected <- test_processed_no_outliers[, c(selected_features)]
train_selected$ocean_proximity_num <- as.numeric(as.factor(train$ocean_proximity))
num_clusters <- 5
clusters <- kmeans(train_selected[, c("longitude", "latitude", "ocean_proximity_num")], centers = num_clusters)
train_selected$position_cluster <- as.factor(clusters$cluster)
clusters <- kmeans(test_selected[, c("longitude", "latitude", "ocean_proximity_num")], centers = num_clusters)
test_selected$position_cluster <- as.factor(clusters$cluster)
calcular_silueta_promedio <- function(data, k) {
set.seed(42)
kmeans_result <- kmeans(data, centers = k)
silueta <- silhouette(kmeans_result$cluster, dist(data))
mean(silueta[, 3])  # Promedio del coeficiente de silueta
}
# Calcular la silueta promedio para k = 2 a 10
k_values <- 2:15
fviz_nbclust(test_selected[, c("longitude", "latitude", "ocean_proximity_num")], kmeans, method = "wss")
silueta_promedio <- sapply(k_values, function(k) calcular_silueta_promedio(test_selected[, c("longitude", "latitude", "ocean_proximity_num")], k))
# Visualizar los resultados
plot(k_values, silueta_promedio, type = "b", pch = 19,
xlab = "Número de Clusters (k)", ylab = "Coeficiente de Silueta Promedio",
main = "Coeficiente de Silueta Promedio para Diferentes k")
calcular_silueta_promedio <- function(data, k) {
set.seed(42)
kmeans_result <- kmeans(data, centers = k)
silueta <- silhouette(kmeans_result$cluster, dist(data))
mean(silueta[, 3])  # Promedio del coeficiente de silueta
}
# Calcular la silueta promedio para k = 2 a 10
k_values <- 2:15
fviz_nbclust(train_selected[, c("longitude", "latitude", "ocean_proximity_num")], kmeans, method = "wss")
silueta_promedio <- sapply(k_values, function(k) calcular_silueta_promedio(train_selected[, c("longitude", "latitude", "ocean_proximity_num")], k))
# Visualizar los resultados
plot(k_values, silueta_promedio, type = "b", pch = 19,
xlab = "Número de Clusters (k)", ylab = "Coeficiente de Silueta Promedio",
main = "Coeficiente de Silueta Promedio para Diferentes k")
num_clusters <- 6
clusters <- kmeans(train_selected[, c("longitude", "latitude", "ocean_proximity_num")], centers = num_clusters)
train_selected$position_cluster <- as.factor(clusters$cluster)
clusters <- kmeans(test_selected[, c("longitude", "latitude", "ocean_proximity_num")], centers = num_clusters)
test_selected$position_cluster <- as.factor(clusters$cluster)
num_clusters <- 6
clusters <- kmeans(train_selected[, c("longitude", "latitude", "ocean_proximity_num")], centers = num_clusters)
train_selected$position_cluster <- as.factor(clusters$cluster)
clusters <- kmeans(test_selected[, c("longitude", "latitude", "ocean_proximity_num")], centers = num_clusters)
test_selected$position_cluster <- as.factor(clusters$cluster)
# Visualizar los resultados
plot(k_values, silueta_promedio, type = "b", pch = 19,
xlab = "Número de Clusters (k)", ylab = "Coeficiente de Silueta Promedio",
main = "Coeficiente de Silueta Promedio para Diferentes k")
num_clusters <- 6
clusters <- kmeans(train_selected[, c("longitude", "latitude", "ocean_proximity_num")], centers = num_clusters)
train_selected$position_cluster <- as.factor(clusters$cluster)
clusters <- kmeans(test_selected[, c("longitude", "latitude", "ocean_proximity_num")], centers = num_clusters)
test_selected$position_cluster <- as.factor(clusters$cluster)
# Visualiza los clusters
ggplot(train_selected, aes(x = longitude, y = latitude, color = position_cluster)) +
geom_point() +
labs(title = "Clustering de casas por coordenadas",
x = "Longitud",
y = "Latitud",
color = "Cluster")
num_clusters <- 5
clusters <- kmeans(train_selected[, c("longitude", "latitude", "ocean_proximity_num")], centers = num_clusters)
train_selected$position_cluster <- as.factor(clusters$cluster)
clusters <- kmeans(test_selected[, c("longitude", "latitude", "ocean_proximity_num")], centers = num_clusters)
test_selected$position_cluster <- as.factor(clusters$cluster)
# Visualiza los clusters
ggplot(train_selected, aes(x = longitude, y = latitude, color = position_cluster)) +
geom_point() +
labs(title = "Clustering de casas por coordenadas",
x = "Longitud",
y = "Latitud",
color = "Cluster")
num_clusters <- 6
clusters <- kmeans(train_selected[, c("longitude", "latitude", "ocean_proximity_num")], centers = num_clusters)
train_selected$position_cluster <- as.factor(clusters$cluster)
clusters <- kmeans(test_selected[, c("longitude", "latitude", "ocean_proximity_num")], centers = num_clusters)
test_selected$position_cluster <- as.factor(clusters$cluster)
# Visualiza los clusters
ggplot(train_selected, aes(x = longitude, y = latitude, color = position_cluster)) +
geom_point() +
labs(title = "Clustering de casas por coordenadas",
x = "Longitud",
y = "Latitud",
color = "Cluster")
num_clusters <- 6
clusters <- kmeans(train_selected, centers = num_clusters)
train_selected$position_cluster <- as.factor(clusters$cluster)
clusters <- kmeans(test_selected, centers = num_clusters)
test_selected$position_cluster <- as.factor(clusters$cluster)
# Visualiza los clusters
ggplot(train_selected, aes(x = longitude, y = latitude, color = position_cluster)) +
geom_point() +
labs(title = "Clustering de casas por coordenadas",
x = "Longitud",
y = "Latitud",
color = "Cluster")
num_clusters <- 6
clusters <- kmeans(train_selected[, c("longitude", "latitude", "ocean_proximity_num")], centers = num_clusters)
train_selected$position_cluster <- as.factor(clusters$cluster)
clusters <- kmeans(test_selected[, c("longitude", "latitude", "ocean_proximity_num")], centers = num_clusters)
test_selected$position_cluster <- as.factor(clusters$cluster)
# Visualiza los clusters
ggplot(train_selected, aes(x = longitude, y = latitude, color = position_cluster)) +
geom_point() +
labs(title = "Clustering de casas por coordenadas",
x = "Longitud",
y = "Latitud",
color = "Cluster")
num_clusters <- 6
clusters <- kmeans(train_selected[, c("longitude", "latitude", "ocean_proximity_num", "median_house_value")], centers = num_clusters)
train_selected$position_cluster <- as.factor(clusters$cluster)
clusters <- kmeans(test_selected[, c("longitude", "latitude", "ocean_proximity_num", "median_house_value")], centers = num_clusters)
num_clusters <- 6
clusters <- kmeans(train_selected[, c("longitude", "latitude", "ocean_proximity_num", "median_house_value")], centers = num_clusters)
train_selected$position_cluster <- as.factor(clusters$cluster)
clusters <- kmeans(test_selected[, c("longitude", "latitude", "ocean_proximity_num")], centers = num_clusters)
test_selected$position_cluster <- as.factor(clusters$cluster)
# Visualiza los clusters
ggplot(train_selected, aes(x = longitude, y = latitude, color = position_cluster)) +
geom_point() +
labs(title = "Clustering de casas por coordenadas",
x = "Longitud",
y = "Latitud",
color = "Cluster")
num_clusters <- 6
clusters <- kmeans(train_selected[, c("longitude", "latitude" "median_house_value")], centers = num_clusters)
num_clusters <- 6
clusters <- kmeans(train_selected[, c("longitude", "latitude", "median_house_value")], centers = num_clusters)
train_selected$position_cluster <- as.factor(clusters$cluster)
clusters <- kmeans(test_selected[, c("longitude", "latitude", "ocean_proximity_num")], centers = num_clusters)
test_selected$position_cluster <- as.factor(clusters$cluster)
# Visualiza los clusters
ggplot(train_selected, aes(x = longitude, y = latitude, color = position_cluster)) +
geom_point() +
labs(title = "Clustering de casas por coordenadas",
x = "Longitud",
y = "Latitud",
color = "Cluster")
num_clusters <- 6
clusters <- kmeans(train_selected[, c("longitude", "latitude", "ocean_proximity_num")], centers = num_clusters)
train_selected$position_cluster <- as.factor(clusters$cluster)
clusters <- kmeans(test_selected[, c("longitude", "latitude", "ocean_proximity_num")], centers = num_clusters)
test_selected$position_cluster <- as.factor(clusters$cluster)
# Visualiza los clusters
ggplot(train_selected, aes(x = longitude, y = latitude, color = position_cluster)) +
geom_point() +
labs(title = "Clustering de casas por coordenadas",
x = "Longitud",
y = "Latitud",
color = "Cluster")
# Seleccionar solo las características seleccionadas para el entrenamiento
set.seed(123)
train_control <- trainControl(method = "cv", number = 5)
# Modelo Random Forest
rf_model <- train(median_house_value ~ ., data = train_selected, method = "rf", trControl = train_control)
print(rf_model)
# Modelo XGBoost
xgb_grid <- expand.grid(
nrounds = 100,
max_depth = 6,
eta = 0.3,
gamma = 0,
colsample_bytree = 1,
min_child_weight = 1,
subsample = 1
)
xgb_model <- train(median_house_value ~ ., data = train_selected, method = "xgbTree", trControl = train_control, tuneGrid = xgb_grid)
print(xgb_model)
# Comparar RMSE de los modelos
results <- resamples(list(rf = rf_model, xgb = xgb_model))
summary(results)
# Seleccionar el mejor modelo basado en el RMSE
best_model <- if(min(rf_model$results$RMSE) < min(xgb_model$results$RMSE)) {
rf_model
} else {
xgb_model
}
# Asegurarse de que las variables seleccionadas estén presentes en el conjunto de prueba
# y eliminar la variable median_house_value
selected_features <- setdiff(selected_features, "median_house_value")
test_selected <- test_processed[, selected_features]
# Asegurarse de que test_selected no contenga factores que no estén en el train
for (col in selected_features) {
if (is.factor(test_selected[[col]])) {
test_selected[[col]] <- factor(test_selected[[col]], levels = levels(train_selected[[col]]))
}
}
# Predicción en el conjunto de prueba
test_predictions <- predict(best_model, newdata = test_selected)
View(test_selected)
View(test_selected)
num_clusters <- 6
clusters <- kmeans(train_selected[, c("longitude", "latitude", "ocean_proximity_num")], centers = num_clusters)
train_selected$position_cluster <- as.factor(clusters$cluster)
clusters <- kmeans(test_selected[, c("longitude", "latitude", "ocean_proximity_num")], centers = num_clusters)
test_selected$position_cluster <- as.factor(clusters$cluster)
# Visualiza los clusters
ggplot(train_selected, aes(x = longitude, y = latitude, color = position_cluster)) +
geom_point() +
labs(title = "Clustering de casas por coordenadas",
x = "Longitud",
y = "Latitud",
color = "Cluster")
# Seleccionar el mejor modelo basado en el RMSE
best_model <- if(min(rf_model$results$RMSE) < min(xgb_model$results$RMSE)) {
rf_model
} else {
xgb_model
}
# Asegurarse de que test_selected no contenga factores que no estén en el train
for (col in selected_features) {
if (is.factor(test_selected[[col]])) {
test_selected[[col]] <- factor(test_selected[[col]], levels = levels(train_selected[[col]]))
}
}
# Predicción en el conjunto de prueba
test_predictions <- predict(best_model, newdata = test_selected)
# Generar el archivo de salida
submission <- data.frame(id = test_processed$id, median_house_value = test_predictions)
write.csv(submission, "submission.csv", row.names = FALSE)
best_model
# Seleccionar el mejor modelo basado en el RMSE
best_model <- if(min(rf_model$results$RMSE) < min(xgb_model$results$RMSE)) {
rf_model
} else {
xgb_model
}
# Asegurarse de que test_selected no contenga factores que no estén en el train
for (col in selected_features) {
if (is.factor(test_selected[[col]])) {
test_selected[[col]] <- factor(test_selected[[col]], levels = levels(train_selected[[col]]))
}
}
# Predicción en el conjunto de prueba
test_predictions <- predict(best_model, newdata = test_selected)
# Generar el archivo de salida
submission <- data.frame(id = test_processed$id, median_house_value = test_predictions)
write.csv(submission, "submission_esperanza_2.csv", row.names = FALSE)
best_model
# Ridge Regression
set.seed(123)
elastic_model <- train(median_house_value ~ ., data = train_selected, method = "elastic", trControl = train_control,
tuneLength = 10)
train_selected[,-"median_house_value"]
train_selected[,c("median_house_value")]
train_selected[,-c("median_house_value")]
train_selected[!"median_house_value"]
train_selected[=!"median_house_value"]
train_selected[,!"median_house_value"]
train_selected$!"median_house_value"
train_selected$"median_house_value"
train_selected$median_house_value
train_selected$-"median_house_value"
train_selected[ , -"median_house_value"]
train_selected[, -"median_house_value"]
subset(train_selected, select = -median_house_value)
# Ridge Regression
set.seed(123)
cv_fit <- cv.glmnet(subset(train_selected, select = -median_house_value), train_selected$median_house_value, alpha = 0.5, n_folds = 5)
subset(train_selected, select = -median_house_value)
train_selected$median_house_value
train_selected["median_house_value"]
# Ridge Regression
set.seed(123)
cv_fit <- cv.glmnet(subset(train_selected, select = -median_house_value), train_selected["median_house_value"], alpha = 0.5, n_folds = 5)
View(train_selected)
subset(train_selected, select = -median_house_value)
# Ridge Regression
set.seed(123)
cv_fit <- cv.glmnet(subset(train_selected, select = -median_house_value), train_selected["median_house_value"], alpha = 0.5, n_folds = 5)
# Ridge Regression
set.seed(123)
cv_fit <- cv.glmnet(subset(train_selected, select = -median_house_value), train_selected["median_house_value"], alpha = 0.5, n_folds = 5)
# Ridge Regression
y = train_selected["median_house_value"],
# Ridge Regression
y = train_selected["median_house_value"]
set.seed(123)
cv_fit <- cv.glmnet(subset(train_selected, select = -median_house_value), y, alpha = 0.5, n_folds = 5)
# Ridge Regression
y = train_selected["median_house_value"]
x=subset(train_selected, select = -median_house_value)
set.seed(123)
cv_fit <- cv.glmnet(, y, alpha = 0.5, n_folds = 5)
# Ridge Regression
y = train_selected["median_house_value"]
x=subset(train_selected, select = -median_house_value)
set.seed(123)
cv_fit <- cv.glmnet(x, y, alpha = 0.5, n_folds = 5)
# Ridge Regression
y = train_selected["median_house_value"]
x = subset(train_selected, select = -median_house_value)
set.seed(123)
cv_fit <- cv.glmnet(x, y, alpha = 0.5, n_folds = 5)
# Ridge Regression
y = train_selected["median_house_value"]
x = subset(train_selected, select = -median_house_value)
if (is.list(y)) {
y <- unlist(y)
print("y ha sido convertido a un vector numérico.")
}
set.seed(123)
cv_fit <- cv.glmnet(x, y, alpha = 0.5, n_folds = 5)
# Ridge Regression
y = train_selected["median_house_value"]
x = subset(train_selected, select = -median_house_value)
elastic_model <- train(median_house_value ~ ., data = train_selected, method = "glmnet", trControl = train_control,
tuneLength = 10)
print(ridge_model)
print(elastic_model)
# Ridge Regression
y = train_selected["median_house_value"]
x = subset(train_selected, select = -median_house_value)
elastic_model <- train(median_house_value ~ ., data = train_selected, method = "glmnet", trControl = train_control,
tuneLength = 10)
print(elastic_model)
# Predicción en el conjunto de prueba
test_predictions <- predict(elastic_model, newdata = test_selected)
# Generar el archivo de salida
submission <- data.frame(id = test_processed$id, median_house_value = test_predictions)
write.csv(submission, "submission_esperanza_2.csv", row.names = FALSE)
