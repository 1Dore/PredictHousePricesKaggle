library(tidyverse)
library(caret)
library(randomForest)
library(xgboost)
library(GGally)
# Cargar los datasets
train <- read.csv("train.csv")
test <- read.csv("test.csv")
# Ver las primeras filas del conjunto de entrenamiento
head(train)
# Resumen de los datos
summary(train)
# Distribución de la variable objetivo
ggplot(train, aes(x = median_house_value)) +
geom_histogram(binwidth = 50000, fill = "blue", color = "black") +
theme_minimal() +
labs(title = "Distribución de median_house_value")
# Correlación entre las variables numéricas
num_vars <- train %>% select(-id, -ocean_proximity)
ggcorr(num_vars, label = TRUE)
# Relación entre median_income y median_house_value
ggplot(train, aes(x = median_income, y = median_house_value)) +
geom_point(alpha = 0.3) +
theme_minimal() +
labs(title = "Relación entre median_income y median_house_value")
# Imputar valores nulos en total_bedrooms
train$total_bedrooms[is.na(train$total_bedrooms)] <- median(train$total_bedrooms, na.rm = TRUE)
test$total_bedrooms[is.na(test$total_bedrooms)] <- median(test$total_bedrooms, na.rm = TRUE)
# Codificar variables categóricas
train$ocean_proximity <- as.factor(train$ocean_proximity)
test$ocean_proximity <- as.factor(test$ocean_proximity)
# Escalar características numéricas
num_features <- names(train)[sapply(train, is.numeric) & names(train) != "median_house_value"]
preProcValues <- preProcess(train[, num_features], method = c("center", "scale"))
train[, num_features] <- predict(preProcValues, train[, num_features])
test[, num_features] <- predict(preProcValues, test[, num_features])
# Utilizar Recursive Feature Elimination (RFE) para seleccionar las mejores variables
set.seed(123)
control <- rfeControl(functions = rfFuncs, method = "cv", number = 5)
rfe_results <- rfe(train[, num_features], train$median_house_value, sizes = c(1:10), rfeControl = control)
# Resumen de las variables seleccionadas
print(rfe_results)
# Variables seleccionadas
selected_features <- predictors(rfe_results)
selected_features
set.seed(123)
train_control <- trainControl(method = "cv", number = 5)
# Modelo Random Forest
rf_model <- train(median_house_value ~ ., data = train, method = "rf", trControl = train_control)
print(rf_model)
# Modelo XGBoost
xgb_grid <- expand.grid(
nrounds = 100,
max_depth = 6,
eta = 0.3,
gamma = 0,
colsample_bytree = 1,
min_child_weight = 1,
subsample = 1
)
xgb_model <- train(median_house_value ~ ., data = train, method = "xgbTree", trControl = train_control, tuneGrid = xgb_grid)
print(xgb_model)
# Comparar RMSE de los modelos
results <- resamples(list(rf = rf_model, xgb = xgb_model))
summary(results)
# Ver las primeras filas del conjunto de entrenamiento
head(train)
# Resumen de los datos
summary(train)
# Distribución de la variable objetivo
ggplot(train, aes(x = median_house_value)) +
geom_histogram(binwidth = 50000, fill = "blue", color = "black") +
theme_minimal() +
labs(title = "Distribución de median_house_value")
# Correlación entre las variables numéricas
num_vars <- train %>% select(-id, -ocean_proximity)
ggcorr(num_vars, label = TRUE)
# Relación entre median_income y median_house_value
ggplot(train, aes(x = median_income, y = median_house_value)) +
geom_point(alpha = 0.3) +
theme_minimal() +
labs(title = "Relación entre median_income y median_house_value")
# Imputar valores nulos en total_bedrooms
train$total_bedrooms[is.na(train$total_bedrooms)] <- median(train$total_bedrooms, na.rm = TRUE)
test$total_bedrooms[is.na(test$total_bedrooms)] <- median(test$total_bedrooms, na.rm = TRUE)
# Codificar variables categóricas
train$ocean_proximity <- as.factor(train$ocean_proximity)
test$ocean_proximity <- as.factor(test$ocean_proximity)
# Escalar características numéricas
num_features <- names(train)[sapply(train, is.numeric) & names(train) != "median_house_value" & names(train) != "id"]
preProcValues <- preProcess(train[, num_features], method = c("center", "scale"))
train[, num_features] <- predict(preProcValues, train[, num_features])
test[, num_features] <- predict(preProcValues, test[, num_features])
# Utilizar Recursive Feature Elimination (RFE) para seleccionar las 5 mejores variables
set.seed(123)
control <- rfeControl(functions = rfFuncs, method = "cv", number = 5)
rfe_results <- rfe(train[, num_features], train$median_house_value, sizes = 5, rfeControl = control)
# Resumen de las variables seleccionadas
print(rfe_results)
# Variables seleccionadas
selected_features <- predictors(rfe_results)
print(selected_features)
# Añadir la variable objetivo a las características seleccionadas
selected_features <- c(selected_features, "median_house_value")
set.seed(123)
train_control <- trainControl(method = "cv", number = 5)
# Modelo Random Forest
rf_model <- train(median_house_value ~ ., data = train[, selected_features], method = "rf", trControl = train_control)
print(rf_model)
# Modelo XGBoost
xgb_grid <- expand.grid(
nrounds = 100,
max_depth = 6,
eta = 0.3,
gamma = 0,
colsample_bytree = 1,
min_child_weight = 1,
subsample = 1
)
xgb_model <- train(median_house_value ~ ., data = train[, selected_features], method = "xgbTree", trControl = train_control, tuneGrid = xgb_grid)
print(xgb_model)
# Comparar RMSE de los modelos
results <- resamples(list(rf = rf_model, xgb = xgb_model))
summary(results)
# Seleccionar el mejor modelo basado en el RMSE
best_model <- ifelse(min(rf_model$results$RMSE) < min(xgb_model$results$RMSE), rf_model, xgb_model)
# Predicción en el conjunto de prueba
test_predictions <- predict(best_model, newdata = test[, selected_features])
# Seleccionar el mejor modelo basado en el RMSE
best_model <- ifelse(min(rf_model$results$RMSE) < min(xgb_model$results$RMSE), rf_model, xgb_model)
# Asegurarse de que las variables seleccionadas estén presentes en el conjunto de prueba
test_selected <- test[, selected_features]
# Seleccionar el mejor modelo basado en el RMSE
best_model <- ifelse(min(rf_model$results$RMSE) < min(xgb_model$results$RMSE), rf_model, xgb_model)
# Asegurarse de que las variables seleccionadas estén presentes en el conjunto de prueba
test_selected <- test[, selected_features]
# Seleccionar el mejor modelo basado en el RMSE
best_model <- ifelse(min(rf_model$results$RMSE) < min(xgb_model$results$RMSE), rf_model, xgb_model)
# Asegurarse de que las variables seleccionadas estén presentes en el conjunto de prueba
# y eliminar la variable median_house_value
selected_features <- setdiff(selected_features, "median_house_value")
test_selected <- test[, selected_features]
# Predicción en el conjunto de prueba
test_predictions <- predict(best_model, newdata = test_selected)
# Añadir la variable objetivo a las características seleccionadas
selected_features <- c(selected_features, "median_house_value")
# Eliminar "median_house_value" de las características seleccionadas para el conjunto de prueba
selected_features <- setdiff(selected_features, "median_house_value")
set.seed(123)
train_control <- trainControl(method = "cv", number = 5)
# Modelo Random Forest en el conjunto de entrenamiento
rf_model <- train(median_house_value ~ ., data = train[, selected_features], method = "rf", trControl = train_control)
# Añadir la variable objetivo a las características seleccionadas
selected_features <- c(selected_features, "median_house_value")
# Eliminar "median_house_value" de las características seleccionadas para el conjunto de prueba
selected_features <- setdiff(selected_features, "median_house_value")
set.seed(123)
train_control <- trainControl(method = "cv", number = 5)
# Modelo Random Forest en el conjunto de entrenamiento
rf_model <- train(median_house_value ~ ., data = train[, selected_features], method = "rf", trControl = train_control)
# Añadir la variable objetivo a las características seleccionadas
selected_features <- c(selected_features, "median_house_value")
# Eliminar "median_house_value" de las características seleccionadas para el conjunto de prueba
selected_features <- setdiff(selected_features, "median_house_value")
set.seed(123)
train_control <- trainControl(method = "cv", number = 5)
# Modelo Random Forest en el conjunto de entrenamiento
rf_model <- train(data = train[, selected_features], method = "rf", trControl = train_control)
# Seleccionar el mejor modelo basado en el RMSE
best_model <- ifelse(min(rf_model$results$RMSE) < min(xgb_model$results$RMSE), rf_model, xgb_model)
# Asegurarse de que las variables seleccionadas estén presentes en el conjunto de prueba
test_selected <- test[, selected_features]
# Predicción en el conjunto de prueba
test_predictions <- predict(best_model, newdata = test_selected)
test
# Imputar valores nulos en total_bedrooms
train$total_bedrooms[is.na(train$total_bedrooms)] <- median(train$total_bedrooms, na.rm = TRUE)
test$total_bedrooms[is.na(test$total_bedrooms)] <- median(test$total_bedrooms, na.rm = TRUE)
# Codificar variables categóricas
train$Ocean_proximity_num <- as.numeric(factor(train$Ocean_proximity))
# Imputar valores nulos en total_bedrooms
train$total_bedrooms[is.na(train$total_bedrooms)] <- median(train$total_bedrooms, na.rm = TRUE)
test$total_bedrooms[is.na(test$total_bedrooms)] <- median(test$total_bedrooms, na.rm = TRUE)
# Obtener los niveles únicos de Ocean proximity en ambos conjuntos
train_levels <- levels(factor(train$Ocean_proximity))
test_levels <- levels(factor(test$Ocean_proximity))
# Asegurar que ambos conjuntos tengan los mismos niveles
all_levels <- union(train_levels, test_levels)
# Aplicar los mismos niveles a ambos conjuntos
train$Ocean_proximity <- factor(train$Ocean_proximity, levels = all_levels)
# Imputar valores nulos en total_bedrooms
train$total_bedrooms[is.na(train$total_bedrooms)] <- median(train$total_bedrooms, na.rm = TRUE)
test$total_bedrooms[is.na(test$total_bedrooms)] <- median(test$total_bedrooms, na.rm = TRUE)
# Ejemplo de datos de entrenamiento y prueba
train <- data.frame(
Ocean_proximity = c("NEAR BAY", "<1H OCEAN", "INLAND", "NEAR BAY", "ISLAND"),
other_column = c(1, 2, 3, 4, 5)  # Otra columna de ejemplo
)
test <- data.frame(
Ocean_proximity = c("<1H OCEAN", "NEAR BAY", "INLAND", "NEAR BAY", "NEAR BAY"),
other_column = c(6, 7, 8, 9, 10)  # Otra columna de ejemplo
)
# Convertir Ocean_proximity a factor para asegurar los niveles
train$Ocean_proximity <- factor(train$Ocean_proximity)
test$Ocean_proximity <- factor(test$Ocean_proximity)
# Transformar Ocean_proximity a números en el conjunto de entrenamiento y prueba
train$Ocean_proximity_num <- as.integer(train$Ocean_proximity)
test$Ocean_proximity_num <- as.integer(test$Ocean_proximity)
# Escalar características numéricas
num_features <- names(train)[sapply(train, is.numeric) & names(train) != "median_house_value" & names(train) != "id"]
preProcValues <- preProcess(train[, num_features], method = c("center", "scale"))
train[, num_features] <- predict(preProcValues, train[, num_features])
test[, num_features] <- predict(preProcValues, test[, num_features])
test
# Imputar valores nulos en total_bedrooms
train$total_bedrooms[is.na(train$total_bedrooms)] <- median(train$total_bedrooms, na.rm = TRUE)
test$total_bedrooms[is.na(test$total_bedrooms)] <- median(test$total_bedrooms, na.rm = TRUE)
# Codificar variables categóricas como números
train$ocean_proximity <- as.integer(factor(train$ocean_proximity))
# Imputar valores nulos en total_bedrooms
train$total_bedrooms[is.na(train$total_bedrooms)] <- median(train$total_bedrooms, na.rm = TRUE)
test$total_bedrooms[is.na(test$total_bedrooms)] <- median(test$total_bedrooms, na.rm = TRUE)
# Transformar ocean_proximity a números en el conjunto de entrenamiento y prueba
train$ocean_proximity_num <- as.integer(factor(train$ocean_proximity, levels = unique(train$ocean_proximity)))
# Cargar los datasets
train <- read.csv("train.csv")
test <- read.csv("test.csv")
# Ver las primeras filas del conjunto de entrenamiento
head(train)
# Resumen de los datos
summary(train)
# Distribución de la variable objetivo
ggplot(train, aes(x = median_house_value)) +
geom_histogram(binwidth = 50000, fill = "blue", color = "black") +
theme_minimal() +
labs(title = "Distribución de median_house_value")
# Correlación entre las variables numéricas
num_vars <- train %>% select(-id, -ocean_proximity)
ggcorr(num_vars, label = TRUE)
# Relación entre median_income y median_house_value
ggplot(train, aes(x = median_income, y = median_house_value)) +
geom_point(alpha = 0.3) +
theme_minimal() +
labs(title = "Relación entre median_income y median_house_value")
# Imputar valores nulos en total_bedrooms
train$total_bedrooms[is.na(train$total_bedrooms)] <- median(train$total_bedrooms, na.rm = TRUE)
test$total_bedrooms[is.na(test$total_bedrooms)] <- median(test$total_bedrooms, na.rm = TRUE)
# Transformar ocean_proximity a números en el conjunto de entrenamiento y prueba
train$ocean_proximity_num <- as.integer(factor(train$ocean_proximity, levels = unique(train$ocean_proximity)))
test$ocean_proximity_num <- as.integer(factor(test$ocean_proximity, levels = unique(test$ocean_proximity)))
# Escalar características numéricas
num_features <- names(train)[sapply(train, is.numeric) & names(train) != "id"]
preProcValues <- preProcess(train[, num_features], method = c("center", "scale"))
train[, num_features] <- predict(preProcValues, train[, num_features])
test[, num_features] <- predict(preProcValues, test[, num_features])
# Imputar valores nulos en total_bedrooms
train$total_bedrooms[is.na(train$total_bedrooms)] <- median(train$total_bedrooms, na.rm = TRUE)
# Cargar los datasets
train <- read.csv("train.csv")
test <- read.csv("test.csv")
# Ver las primeras filas del conjunto de entrenamiento
head(train)
# Resumen de los datos
summary(train)
# Distribución de la variable objetivo
ggplot(train, aes(x = median_house_value)) +
geom_histogram(binwidth = 50000, fill = "blue", color = "black") +
theme_minimal() +
labs(title = "Distribución de median_house_value")
# Correlación entre las variables numéricas
num_vars <- train %>% select(-id, -ocean_proximity)
ggcorr(num_vars, label = TRUE)
# Relación entre median_income y median_house_value
ggplot(train, aes(x = median_income, y = median_house_value)) +
geom_point(alpha = 0.3) +
theme_minimal() +
labs(title = "Relación entre median_income y median_house_value")
# Imputar valores nulos en total_bedrooms
train$total_bedrooms[is.na(train$total_bedrooms)] <- median(train$total_bedrooms, na.rm = TRUE)
test$total_bedrooms[is.na(test$total_bedrooms)] <- median(test$total_bedrooms, na.rm = TRUE)
# Transformar ocean_proximity a números en el conjunto de entrenamiento y prueba
train$ocean_proximity_num <- as.factor(datos$ocean_proximity)
# Imputar valores nulos en total_bedrooms
train$total_bedrooms[is.na(train$total_bedrooms)] <- median(train$total_bedrooms, na.rm = TRUE)
test$total_bedrooms[is.na(test$total_bedrooms)] <- median(test$total_bedrooms, na.rm = TRUE)
# Transformar ocean_proximity a números en el conjunto de entrenamiento y prueba
train$ocean_proximity_num <- as.factor(train$ocean_proximity)
test$ocean_proximity_num <- as.factor(test$ocean_proximity)
# Escalar características numéricas
num_features <- names(train)[sapply(train, is.numeric) & names(train) != "id"]
preProcValues <- preProcess(train[, num_features], method = c("center", "scale"))
train[, num_features] <- predict(preProcValues, train[, num_features])
test[, num_features] <- predict(preProcValues, test[, num_features])
# Cargar los datasets
train <- read.csv("train.csv")
test <- read.csv("test.csv")
# Ver las primeras filas del conjunto de entrenamiento
head(train)
# Resumen de los datos
summary(train)
# Distribución de la variable objetivo
ggplot(train, aes(x = median_house_value)) +
geom_histogram(binwidth = 50000, fill = "blue", color = "black") +
theme_minimal() +
labs(title = "Distribución de median_house_value")
# Correlación entre las variables numéricas
num_vars <- train %>% select(-id, -ocean_proximity)
ggcorr(num_vars, label = TRUE)
# Relación entre median_income y median_house_value
ggplot(train, aes(x = median_income, y = median_house_value)) +
geom_point(alpha = 0.3) +
theme_minimal() +
labs(title = "Relación entre median_income y median_house_value")
# Crear copias de train y test originales
train_processed <- train
test_processed <- test
# Imputar valores nulos en total_bedrooms
train_processed$total_bedrooms[is.na(train_processed$total_bedrooms)] <- median(train_processed$total_bedrooms, na.rm = TRUE)
test_processed$total_bedrooms[is.na(test_processed$total_bedrooms)] <- median(test_processed$total_bedrooms, na.rm = TRUE)
# Codificar variables categóricas en ocean_proximity
train_processed$ocean_proximity <- as.factor(train_processed$ocean_proximity)
test_processed$ocean_proximity <- as.factor(test_processed$ocean_proximity)
# Escalar características numéricas
num_features <- names(train_processed)[sapply(train_processed, is.numeric) & names(train_processed) != "median_house_value"]
preProcValues <- preProcess(train_processed[, num_features], method = c("center", "scale"))
train_processed[, num_features] <- predict(preProcValues, train_processed[, num_features])
test_processed[, num_features] <- predict(preProcValues, test_processed[, num_features])
# Verificar los nuevos dataframes train_processed y test_processed
head(train_processed)
head(test_processed)
# Crear copias de train y test originales
train_processed <- train
test_processed <- test
# Imputar valores nulos en total_bedrooms
train_processed$total_bedrooms[is.na(train_processed$total_bedrooms)] <- median(train_processed$total_bedrooms, na.rm = TRUE)
test_processed$total_bedrooms[is.na(test_processed$total_bedrooms)] <- median(test_processed$total_bedrooms, na.rm = TRUE)
# Codificar variables categóricas en ocean_proximity como dummy variables
train_processed <- cbind(train_processed, model.matrix(~ ocean_proximity - 1, data = train_processed))
test_processed <- cbind(test_processed, model.matrix(~ ocean_proximity - 1, data = test_processed))
# Eliminar la columna original ocean_proximity
train_processed <- train_processed[, !names(train_processed) %in% "ocean_proximity"]
test_processed <- test_processed[, !names(test_processed) %in% "ocean_proximity"]
# Escalar características numéricas
num_features <- names(train_processed)[sapply(train_processed, is.numeric) & names(train_processed) != "median_house_value"]
preProcValues <- preProcess(train_processed[, num_features], method = c("center", "scale"))
train_processed[, num_features] <- predict(preProcValues, train_processed[, num_features])
test_processed[, num_features] <- predict(preProcValues, test_processed[, num_features])
# Verificar los nuevos dataframes train_processed y test_processed
head(train_processed)
head(test_processed)
# Utilizar Recursive Feature Elimination (RFE) para seleccionar las 5 mejores variables
set.seed(123)
control <- rfeControl(functions = rfFuncs, method = "cv", number = 5)
rfe_results <- rfe(train_processed[, num_features], train_processed$median_house_value, sizes = 5, rfeControl = control)
# Resumen de las variables seleccionadas
print(rfe_results)
# Variables seleccionadas
selected_features <- predictors(rfe_results)
print(selected_features)
