limite_superior <- Q3 + 1.5 * IQR
# Identificar los outliers
outliers <- vector_numerico[vector_numerico < limite_inferior | vector_numerico > limite_superior]
# Contar los outliers
conteo_outliers <- length(outliers)
# Resultados
print(column_name)
print(paste("Número de outliers identificados usando IQR:", conteo_outliers))
print(paste("Prosnetaje de outliers identificados usando IQR:", conteo_outliers/nrow(train)))
}
chart.Correlation(train_processed_no_outliers[continuas], histogram=TRUE)
train_processed_no_outliers
remove_outliers <- function(df, columns) {
for (col in columns) {
print(col)
Q1 <- quantile(df[[col]], 0.25)
Q3 <- quantile(df[[col]], 0.75)
IQR <- Q3 - Q1
df <- df[!(df[[col]] < (Q1 - 1.5 * IQR) | df[[col]] > (Q3 + 1.5 * IQR)), ]
}
return(df)
}
train_processed_no_outliers <- remove_outliers(train_processed, names(continuas[continuas]))
cat("Tamaño del dataset train_processed original: ", nrow(train_processed), "\n")
cat("Tamaño del dataset train_processed sin outliers: ", nrow(train_processed_no_outliers), "\n")
names(continuas[continuas])
names(continuas)
remove_outliers <- function(df, columns) {
for (col in columns) {
print(col)
Q1 <- quantile(df[[col]], 0.25)
Q3 <- quantile(df[[col]], 0.75)
IQR <- Q3 - Q1
df <- df[!(df[[col]] < (Q1 - 1.5 * IQR) | df[[col]] > (Q3 + 1.5 * IQR)), ]
}
return(df)
}
train_processed_no_outliers <- remove_outliers(train_processed, continuas
cat("Tamaño del dataset train_processed original: ", nrow(train_processed), "\n")
remove_outliers <- function(df, columns) {
for (col in columns) {
print(col)
Q1 <- quantile(df[[col]], 0.25)
Q3 <- quantile(df[[col]], 0.75)
IQR <- Q3 - Q1
df <- df[!(df[[col]] < (Q1 - 1.5 * IQR) | df[[col]] > (Q3 + 1.5 * IQR)), ]
}
return(df)
}
train_processed_no_outliers <- remove_outliers(train_processed, continuas)
cat("Tamaño del dataset train_processed original: ", nrow(train_processed), "\n")
cat("Tamaño del dataset train_processed sin outliers: ", nrow(train_processed_no_outliers), "\n")
remove_outliers <- function(df, columns) {
for (col in columns) {
Q1 <- quantile(df[[col]], 0.25)
Q3 <- quantile(df[[col]], 0.75)
IQR <- Q3 - Q1
df <- df[!(df[[col]] < (Q1 - 1.5 * IQR) | df[[col]] > (Q3 + 1.5 * IQR)), ]
}
return(df)
}
train_processed_no_outliers <- remove_outliers(train_processed, continuas)
cat("Tamaño del dataset train_processed original: ", nrow(train_processed), "\n")
cat("Tamaño del dataset train_processed sin outliers: ", nrow(train_processed_no_outliers), "\n")
ggplot(train_processed_no_outliers, aes(x = longitude, y = latitude, color = position_cluster)) +
geom_point() +
labs(title = "Clustering de casas por coordenadas",
x = "Longitud",
y = "Latitud",
color = "Cluster")
for (column_name  in  continuas) {
vector_numerico = unlist(train_processed_no_outliers[column_name])
Q1 <- quantile(vector_numerico, 0.25)  # Primer cuartil (Q1)
Q3 <- quantile(vector_numerico, 0.75)  # Tercer cuartil (Q3)
IQR <- Q3 - Q1  # Rango intercuartílico
# Definir los límites inferior y superior para identificar outliers
limite_inferior <- Q1 - 1.5 * IQR
limite_superior <- Q3 + 1.5 * IQR
# Identificar los outliers
outliers <- vector_numerico[vector_numerico < limite_inferior | vector_numerico > limite_superior]
# Contar los outliers
conteo_outliers <- length(outliers)
# Resultados
print(column_name)
print(paste("Número de outliers identificados usando IQR:", conteo_outliers))
print(paste("Prosnetaje de outliers identificados usando IQR:", conteo_outliers/nrow(train)))
}
chart.Correlation(train_processed_no_outliers[continuas], histogram=TRUE)
head(train_processed_no_outliers)
# Utilizar Recursive Feature Elimination (RFE) para seleccionar las 5 mejores variables
set.seed(123)
control <- rfeControl(functions = rfFuncs, method = "cv", number = 6)
rfe_results <- rfe(train_processed_no_outliers[, num_features], train_processed_no_outliers$median_house_value, sizes = 1:15, rfeControl = control)
# Resumen de las variables seleccionadas
print(rfe_results)
# Variables seleccionadas
selected_features <- predictors(rfe_results)
print(selected_features)
# Guardar los datos seleccionados
# train_selected <- train_processed[, c(selected_features, "median_house_value")]
# write.csv(train_selected, "train_selected.csv", row.names = FALSE)
selected_features
test_selected <- test_processed_no_outliers[, c(selected_features)]
# Seleccionar solo las características seleccionadas para el entrenamiento
train_selected <- train_processed_no_outliers[, c(selected_features, "median_house_value", "position_cluster")]
test_selected <- test_processed_no_outliers[, c(selected_features, "position_cluster")]
calcular_silueta_promedio <- function(data, k) {
set.seed(42)
kmeans_result <- kmeans(data, centers = k)
silueta <- silhouette(kmeans_result$cluster, dist(data))
mean(silueta[, 3])  # Promedio del coeficiente de silueta
}
# Calcular la silueta promedio para k = 2 a 10
k_values <- 2:15
fviz_nbclust(train_selected[, c("longitude", "latitude", "ocean_proximity_num")], kmeans, method = "wss")
silueta_promedio <- sapply(k_values, function(k) calcular_silueta_promedio(train_selected[, c("longitude", "latitude", "ocean_proximity_num")], k))
# Visualizar los resultados
plot(k_values, silueta_promedio, type = "b", pch = 19,
xlab = "Número de Clusters (k)", ylab = "Coeficiente de Silueta Promedio",
main = "Coeficiente de Silueta Promedio para Diferentes k")
num_clusters <- 4
clusters <- kmeans(train_selected[, c("longitude", "latitude", "ocean_proximity_num")], centers = num_clusters)
train_selected$position_cluster <- as.factor(clusters$cluster)
clusters <- kmeans(test_selected[, c("longitude", "latitude", "ocean_proximity_num")], centers = num_clusters)
test_selected$position_cluster <- as.factor(clusters$cluster)
# Visualiza los clusters
ggplot(train_selected, aes(x = longitude, y = latitude, color = position_cluster)) +
geom_point() +
labs(title = "Clustering de casas por coordenadas",
x = "Longitud",
y = "Latitud",
color = "Cluster")
# Hyperparameter tuning for XGBoost with more extensive grid search
xgb_grid_2 <- expand.grid(
nrounds = c(100, 200),
max_depth = c(6, 10),
eta = c(0.01, 0.1, 0.3),
gamma = c(0, 0.1, 0.2),
colsample_bytree = c(0.8, 1),
min_child_weight = c(1, 3),
subsample = c(0.8, 1)
tuneLength = 10,
# Hyperparameter tuning for XGBoost with more extensive grid search
xgb_grid_2 <- expand.grid(
nrounds = c(100, 200),
max_depth = c(6, 10),
eta = c(0.01, 0.1, 0.3),
gamma = c(0, 0.1, 0.2),
colsample_bytree = c(0.8, 1),
min_child_weight = c(1, 3),
subsample = c(0.8, 1)
tuneLength = c(10, 5),
# Hyperparameter tuning for XGBoost with more extensive grid search
xgb_grid_2 <- expand.grid(
nrounds = c(100, 200),
max_depth = c(6, 10),
eta = c(0.01, 0.1, 0.3),
gamma = c(0, 0.1, 0.2),
colsample_bytree = c(0.8, 1),
min_child_weight = c(1, 3),
subsample = c(0.8, 1),
tuneLength = 10,
metric = "RMSE",
early_stopping_rounds = 10,  # Número de iteraciones sin mejora para detener el entrenamiento
watchlist = list(train = trainData, val = valData)  # Conjunto de validación
)
# Hyperparameter tuning for XGBoost with more extensive grid search
xgb_grid_2 <- expand.grid(
nrounds = c(100, 200),
max_depth = c(6, 10),
eta = c(0.01, 0.1, 0.3),
gamma = c(0, 0.1, 0.2),
colsample_bytree = c(0.8, 1),
min_child_weight = c(1, 3),
subsample = c(0.8, 1),
tuneLength = 10,
metric = "RMSE",
early_stopping_rounds = 10,  # Número de iteraciones sin mejora para detener el entrenamiento
watchlist = list(train = train_selected, val = test_selected)  # Conjunto de validación
)
# Custom train function for xgbTree to avoid the warning
xgb_custom <- trainControl(method = "cv", number = 5)
# Define the train function with additional control to avoid the warning
xgb_model_2 <- train(median_house_value ~ ., data = train_selected, method = "xgbTree",
trControl = xgb_custom, tuneGrid = xgb_grid_2,
verbose = FALSE, nthread = 1)
# Hyperparameter tuning for XGBoost with more extensive grid search
xgb_grid_2 <- expand.grid(
nrounds = c(100, 200),
max_depth = c(6, 10),
eta = c(0.01, 0.1, 0.3),
gamma = c(0, 0.1, 0.2),
colsample_bytree = c(0.8, 1),
min_child_weight = c(1, 3),
subsample = c(0.8, 1),
tuneLength = 10,
metric = "RMSE",
early_stopping_rounds = 10,  # Número de iteraciones sin mejora para detener el entrenamiento
watchlist = list(train = train_selected, val = test_selected)  # Conjunto de validación
)
# Custom train function for xgbTree to avoid the warning
xgb_custom <- trainControl(method = "cv", number = 5)
# Define the train function with additional control to avoid the warning
xgb_model_2 <- train(median_house_value ~ ., data = train_selected, method = "xgbTree",
trControl = xgb_custom, tuneGrid = xgb_grid_2,
verbose = FALSE, nthread = 1)
# Hyperparameter tuning for XGBoost with more extensive grid search
xgb_grid_2 <- expand.grid(
nrounds = c(100, 200),
max_depth = c(6, 10),
eta = c(0.01, 0.1, 0.3),
gamma = c(0, 0.1, 0.2),
colsample_bytree = c(0.8, 1),
min_child_weight = c(1, 3),
subsample = c(0.8, 1)
)
# Custom train function for xgbTree to avoid the warning
xgb_custom <- trainControl(method = "cv", number = 5)
# Define the train function with additional control to avoid the warning
xgb_model_2 <- train(median_house_value ~ ., data = train_selected, method = "xgbTree",
trControl = xgb_custom, tuneGrid = xgb_grid_2,
verbose = FALSE, nthread = 1)
print(xgb_model_2)
# Predicción en el conjunto de prueba
test_predictions <- predict(xgb_model_2, newdata = test_selected)
# Generar el archivo de salida
submission <- data.frame(id = test_processed$id, median_house_value = test_predictions)
# Predicción en el conjunto de prueba
test_predictions <- predict(xgb_model_2, newdata = test_selected)
# Generar el archivo de salida
submission <- data.frame(id = test_selected$id, median_house_value = test_predictions)
View(train_selected)
# Cargar los datasets
train <- read.csv("../train.csv")
test <- read.csv("../test.csv")
# Seleccionar solo las características seleccionadas para el entrenamiento
train$ocean_proximity_num <- as.numeric(as.factor(train$ocean_proximity))
test$ocean_proximity_num <- as.numeric(as.factor(test$ocean_proximity))
num_clusters <- 5
clusters <- kmeans(train[, c("longitude", "latitude", "ocean_proximity_num")], centers = num_clusters)
train$position_cluster <- as.factor(clusters$cluster)
clusters <- kmeans(test[, c("longitude", "latitude", "ocean_proximity_num")], centers = num_clusters)
test$position_cluster <- as.factor(clusters$cluster)
ggplot(train_processed_no_outliers, aes(x = longitude, y = latitude, color = position_cluster)) +
geom_point() +
labs(title = "Clustering de casas por coordenadas",
x = "Longitud",
y = "Latitud",
color = "Cluster")
# Imputar valores nulos en total_bedrooms
train$total_bedrooms[is.na(train$total_bedrooms)] <- median(train$total_bedrooms, na.rm = TRUE)
test$total_bedrooms[is.na(test$total_bedrooms)] <- median(test$total_bedrooms, na.rm = TRUE)
# Codificar variables categóricas
train$ocean_proximity <- as.factor(train$ocean_proximity)
test$ocean_proximity <- as.factor(test$ocean_proximity)
# Escalar características numéricas
num_features <- names(train)[sapply(train, is.numeric) & names(train) != "median_house_value" & names(train) != "id"]
preProcValues <- preProcess(train[, num_features], method = c("center", "scale"))
train_processed <- train
train_processed[, num_features] <- predict(preProcValues, train[, num_features])
test_processed <- test
test_processed[, num_features] <- predict(preProcValues, test[, num_features])
# Guardar los nuevos datasets procesados
# write.csv(train_processed, "train_processed.csv", row.names = FALSE)
# write.csv(test_processed, "test_processed.csv", row.names = FALSE)
continuas =  names(train)[sapply(train, is.numeric) & names(train) != "median_house_value" & names(train) != "id"  &   30 < sapply(train, n_distinct)  ]
continuas
remove_outliers <- function(df, columns) {
for (col in columns) {
Q1 <- quantile(df[[col]], 0.25)
Q3 <- quantile(df[[col]], 0.75)
IQR <- Q3 - Q1
df <- df[!(df[[col]] < (Q1 - 1.5 * IQR) | df[[col]] > (Q3 + 1.5 * IQR)), ]
}
return(df)
}
train_processed_no_outliers <- remove_outliers(train_processed, continuas)
cat("Tamaño del dataset train_processed original: ", nrow(train_processed), "\n")
cat("Tamaño del dataset train_processed sin outliers: ", nrow(train_processed_no_outliers), "\n")
selected_features
# Seleccionar solo las características seleccionadas para el entrenamiento
train_selected <- train_processed_no_outliers[, c(selected_features, "median_house_value", "position_cluster")]
test_selected <- test_processed_no_outliers[, c(selected_features, "position_cluster")]
num_clusters <- 4
clusters <- kmeans(train_selected[, c("longitude", "latitude", "ocean_proximity_num")], centers = num_clusters)
train_selected$position_cluster <- as.factor(clusters$cluster)
clusters <- kmeans(test_selected[, c("longitude", "latitude", "ocean_proximity_num")], centers = num_clusters)
test_selected$position_cluster <- as.factor(clusters$cluster)
# Visualiza los clusters
ggplot(train_selected, aes(x = longitude, y = latitude, color = position_cluster)) +
geom_point() +
labs(title = "Clustering de casas por coordenadas",
x = "Longitud",
y = "Latitud",
color = "Cluster")
View(test_processed)
View(test_processed_no_outliers)
# Seleccionar solo las características seleccionadas para el entrenamiento
train_selected <- train_processed_no_outliers[, c(selected_features, "median_house_value", "position_cluster")]
test_selected <- test_processed[, c(selected_features, "position_cluster")]
num_clusters <- 4
clusters <- kmeans(train_selected[, c("longitude", "latitude", "ocean_proximity_num")], centers = num_clusters)
train_selected$position_cluster <- as.factor(clusters$cluster)
clusters <- kmeans(test_selected[, c("longitude", "latitude", "ocean_proximity_num")], centers = num_clusters)
test_selected$position_cluster <- as.factor(clusters$cluster)
# Visualiza los clusters
ggplot(train_selected, aes(x = longitude, y = latitude, color = position_cluster)) +
geom_point() +
labs(title = "Clustering de casas por coordenadas",
x = "Longitud",
y = "Latitud",
color = "Cluster")
# Predicción en el conjunto de prueba
test_predictions <- predict(xgb_model_2, newdata = test_selected)
# Generar el archivo de salida
submission <- data.frame(id = test_processed$id, median_house_value = test_predictions)
write.csv(submission, "submission_esperanza_3.csv", row.names = FALSE)
library(tidyverse)
library(caret)
library(randomForest)
library(xgboost)
library(GGally)
library(leaflet)
library(cluster)
library(factoextra)
library(glmnet)
library(PerformanceAnalytics)
# Cargar los datasets
train <- read.csv("../train.csv")
test <- read.csv("../test.csv")
# Ver las primeras filas del conjunto de entrenamiento
head(train)
# Resumen de los datos
summary(train)
# Distribución de la variable objetivo
ggplot(train, aes(x = median_house_value)) +
geom_histogram(binwidth = 50000, fill = "blue", color = "black") +
theme_minimal() +
labs(title = "Distribución de median_house_value")
# Correlación entre las variables numéricas
num_vars <- train %>% select(-id, -ocean_proximity)
ggcorr(num_vars, label = TRUE)
# Relación entre median_income y median_house_value
ggplot(train, aes(x = median_income, y = median_house_value)) +
geom_point(alpha = 0.3) +
theme_minimal() +
labs(title = "Relación entre median_income y median_house_value")
ggplot(train, aes(x = longitude, y = latitude, color = median_house_value)) +
geom_point() +
scale_color_gradient(low = "blue", high = "red") +
labs(title = "Mapa de precios de casas por coordenadas",
x = "Longitud",
y = "Latitud",
color = "Precio de la Casa")
ggplot(train, aes(x = longitude, y = latitude, color = ocean_proximity)) +
geom_point()  +
labs(title = "Mapa de precios de casas por coordenadas",
x = "Longitud",
y = "Latitud",
color = "Cercania")
m <- leaflet(train) %>%
addTiles() %>%
addCircleMarkers(~longitude, ~latitude, radius = ~median_house_value/100000,
color = ~ifelse(median_house_value > median(train$median_house_value), 'red', 'blue'),
popup = ~paste("median_house_value:", median_house_value))
m
calcular_silueta_promedio <- function(data, k) {
set.seed(42)
kmeans_result <- kmeans(data, centers = k)
silueta <- silhouette(kmeans_result$cluster, dist(data))
mean(silueta[, 3])  # Promedio del coeficiente de silueta
}
# Calcular la silueta promedio para k = 2 a 10
k_values <- 2:15
fviz_nbclust(train[, c("longitude", "latitude", "median_house_value")], kmeans, method = "wss")
silueta_promedio <- sapply(k_values, function(k) calcular_silueta_promedio(train[, c("longitude", "latitude", "median_house_value")], k))
# Visualizar los resultados
plot(k_values, silueta_promedio, type = "b", pch = 19,
xlab = "Número de Clusters (k)", ylab = "Coeficiente de Silueta Promedio",
main = "Coeficiente de Silueta Promedio para Diferentes k")
# Establece el número de clusters
num_clusters <- 4
# Aplica el clustering K-Means
set.seed(42) # Para reproducibilidad
clusters <- kmeans(train[, c("longitude", "latitude", "median_house_value")], centers = num_clusters)
# Añade los clusters al dataframe
train$position_cluster <- as.factor(clusters$cluster)
# Visualiza los clusters
ggplot(train, aes(x = longitude, y = latitude, color = position_cluster)) +
geom_point() +
labs(title = "Clustering de casas por coordenadas",
x = "Longitud",
y = "Latitud",
color = "Cluster")
# Establece el número de clusters
num_clusters <- 6
# Aplica el clustering K-Means
set.seed(42) # Para reproducibilidad
clusters <- kmeans(train[, c("longitude", "latitude", "median_house_value")], centers = num_clusters)
# Añade los clusters al dataframe
train$position_cluster <- as.factor(clusters$cluster)
# Visualiza los clusters
ggplot(train, aes(x = longitude, y = latitude, color = position_cluster)) +
geom_point() +
labs(title = "Clustering de casas por coordenadas",
x = "Longitud",
y = "Latitud",
color = "Cluster")
# Establece el número de clusters
num_clusters <- 5
# Aplica el clustering K-Means
set.seed(42) # Para reproducibilidad
clusters <- kmeans(train[, c("longitude", "latitude", "median_house_value")], centers = num_clusters)
# Añade los clusters al dataframe
train$position_cluster <- as.factor(clusters$cluster)
# Visualiza los clusters
ggplot(train, aes(x = longitude, y = latitude, color = position_cluster)) +
geom_point() +
labs(title = "Clustering de casas por coordenadas",
x = "Longitud",
y = "Latitud",
color = "Cluster")
train_selected = subset(train_selected, select = -c("longitude", "latitude", "ocean_proximity_num"))
train_selected = subset(train_selected, select = -("longitude", "latitude", "ocean_proximity_num"))
train_selected = subset(train_selected, select = -{"longitude", "latitude", "ocean_proximity_num"})
subset(train_selected, select = -["longitude", "latitude", "ocean_proximity_num"])
train_selected = subset(train_selected, select = c("longitude", "latitude", "ocean_proximity_num"))
# Seleccionar solo las características seleccionadas para el entrenamiento
train_selected <- train_processed_no_outliers[, c(selected_features, "median_house_value", "position_cluster")]
test_selected <- test_processed[, c(selected_features, "position_cluster")]
subset(train_selected, select = c("longitude", "latitude", "ocean_proximity_num"))
subset(train_selected, unselect = c("longitude", "latitude", "ocean_proximity_num"))
subset(train_selected, select = -c("longitude", "latitude", "ocean_proximity_num"))
subset(train_selected, select = -("longitude", "latitude", "ocean_proximity_num"))
subset(train_selected, select = -"longitude", "latitude", "ocean_proximity_num")
subset(train_selected, select = -"longitude")
subset(train_selected, select = -longitude)
trian_reducre = subset(train_selected, select = -longitude)
trian_reducre = subset(trian_reducre, select = -longitude)
trian_reducre = subset(train_selected, select = -longitude)
trian_reducre = subset(trian_reducre, select = -latitude)
trian_reducre = subset(trian_reducre, select = -ocean_proximity_num)
trian_reducre
# Hyperparameter tuning for XGBoost with more extensive grid search
xgb_grid_2 <- expand.grid(
nrounds = c(100, 200),
max_depth = c(6, 10),
eta = c(0.01, 0.1, 0.3),
gamma = c(0, 0.1, 0.2),
colsample_bytree = c(0.8, 1),
min_child_weight = c(1, 3),
subsample = c(0.8, 1)
)
# Custom train function for xgbTree to avoid the warning
xgb_custom <- trainControl(method = "cv", number = 5)
# Define the train function with additional control to avoid the warning
xgb_model_2 <- train(median_house_value ~ ., data = trian_reducre, method = "xgbTree",
trControl = xgb_custom, tuneGrid = xgb_grid_2,
verbose = FALSE, nthread = 1)
print(xgb_model_2)
# Predicción en el conjunto de prueba
test_predictions <- predict(xgb_model_2, newdata = test_selected)
# Generar el archivo de salida
submission <- data.frame(id = test_processed$id, median_house_value = test_predictions)
write.csv(submission, "submission_esperanza_4.csv", row.names = FALSE)
# Instalar y cargar las librerías necesarias
install.packages(c("randomForest", "xgboost", "GGally"))
library(tidyverse)
library(caret)
library(randomForest)
library(xgboost)
library(GGally)
# Cargar los datasets
train <- read.csv("train.csv")
test <- read.csv("test.csv")
# Ver las primeras filas del conjunto de entrenamiento
head(train)
# Resumen de los datos
summary(train)
# Distribución de la variable objetivo
ggplot(train, aes(x = median_house_value)) +
geom_histogram(binwidth = 50000, fill = "blue", color = "black") +
theme_minimal() +
labs(title = "Distribución de median_house_value")
# Correlación entre las variables numéricas
num_vars <- train %>% select(-id, -ocean_proximity)
ggcorr(num_vars, label = TRUE)
# Relación entre median_income y median_house_value
ggplot(train, aes(x = median_income, y = median_house_value)) +
geom_point(alpha = 0.3) +
theme_minimal() +
labs(title = "Relación entre median_income y median_house_value")
# Imputar valores nulos en total_bedrooms
train$total_bedrooms[is.na(train$total_bedrooms)] <- median(train$total_bedrooms, na.rm = TRUE)
test$total_bedrooms[is.na(test$total_bedrooms)] <- median(test$total_bedrooms, na.rm = TRUE)
# Codificar variables categóricas
train$ocean_proximity <- as.factor(train$ocean_proximity)
test$ocean_proximity <- as.factor(test$ocean_proximity)
# Escalar características numéricas
num_features <- names(train)[sapply(train, is.numeric) & names(train) != "median_house_value" & names(train) != "id"]
preProcValues <- preProcess(train[, num_features], method = c("center", "scale"))
train_processed <- train
train_processed[, num_features] <- predict(preProcValues, train[, num_features])
test_processed <- test
test_processed[, num_features] <- predict(preProcValues, test[, num_features])
# Guardar los nuevos datasets procesados
# write.csv(train_processed, "train_processed.csv", row.names = FALSE)
# write.csv(test_processed, "test_processed.csv", row.names = FALSE)
# Utilizar Recursive Feature Elimination (RFE) para seleccionar las 5 mejores variables
set.seed(123)
control <- rfeControl(functions = rfFuncs, method = "cv", number = 5)
rfe_results <- rfe(train_processed[, num_features], train_processed$median_house_value, sizes = 5, rfeControl = control)
