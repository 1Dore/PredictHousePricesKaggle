---
title: "R Notebook"
output: html_notebook
---


```{r}
# Cargar librerías necesarias
library(tidyverse)
library(caret)
library(lightgbm)
library(GGally)
library(caretEnsemble)
library(data.table)

# Cargar los datasets
train <- fread("train.csv")
test <- fread("test.csv")

# Convertir a data.frame
train <- as.data.frame(train)
test <- as.data.frame(test)

# Imputar valores nulos en total_bedrooms
train$total_bedrooms[is.na(train$total_bedrooms)] <- median(train$total_bedrooms, na.rm = TRUE)
test$total_bedrooms[is.na(test$total_bedrooms)] <- median(test$total_bedrooms, na.rm = TRUE)

# Codificar variables categóricas
train$ocean_proximity <- as.factor(train$ocean_proximity)
test$ocean_proximity <- as.factor(test$ocean_proximity)

# Ingeniería de características
train <- train %>%
  mutate(
    rooms_per_household = total_rooms / households,
    bedrooms_per_room = total_bedrooms / total_rooms,
    population_per_household = population / households,
    income_per_population = median_income / population,
    bedrooms_per_household = total_bedrooms / households
  )

test <- test %>%
  mutate(
    rooms_per_household = total_rooms / households,
    bedrooms_per_room = total_bedrooms / total_rooms,
    population_per_household = population / households,
    income_per_population = median_income / population,
    bedrooms_per_household = total_bedrooms / households
  )

# Escalar características numéricas
num_features <- names(train)[sapply(train, is.numeric) & names(train) != "median_house_value" & names(train) != "id"]
preProcValues <- preProcess(train[, num_features], method = c("center", "scale"))
train_processed <- train
train_processed[, num_features] <- predict(preProcValues, train[, num_features])
test_processed <- test
test_processed[, num_features] <- predict(preProcValues, test[, num_features])

# Seleccionar características
selected_features <- c(num_features, "ocean_proximity", "rooms_per_household", "bedrooms_per_room", "population_per_household", "income_per_population", "bedrooms_per_household")

# Preparar los datos para LightGBM
dtrain <- lgb.Dataset(data = as.matrix(train_processed[, selected_features]), label = train_processed$median_house_value)

# Parámetros de LightGBM
params <- list(
  objective = "regression",
  metric = "rmse",
  boosting_type = "gbdt",
  learning_rate = 0.01,
  num_leaves = 31,
  feature_fraction = 0.9,
  bagging_fraction = 0.8,
  bagging_freq = 5,
  verbose = -1
)

# Realizar una búsqueda de hiperparámetros más exhaustiva
grid <- expand.grid(
  num_leaves = c(31, 50, 70, 90),
  learning_rate = c(0.01, 0.03, 0.05, 0.1),
  feature_fraction = c(0.7, 0.8, 0.9),
  bagging_fraction = c(0.7, 0.8, 0.9),
  bagging_freq = c(5, 10)
)

best_rmse <- Inf
best_params <- list()

for (i in 1:nrow(grid)) {
  params <- list(
    objective = "regression",
    metric = "rmse",
    boosting_type = "gbdt",
    learning_rate = grid$learning_rate[i],
    num_leaves = grid$num_leaves[i],
    feature_fraction = grid$feature_fraction[i],
    bagging_fraction = grid$bagging_fraction[i],
    bagging_freq = grid$bagging_freq[i],
    verbose = -1
  )
  
  set.seed(123)
  model_lgb <- lgb.train(params, dtrain, 1000, valids = list(train = dtrain), early_stopping_rounds = 100, verbose = -1)
  
  rmse <- min(unlist(model_lgb$record_evals$train$rmse$eval))
  if (rmse < best_rmse) {
    best_rmse <- rmse
    best_params <- params
  }
}

# Entrenar el modelo final con los mejores parámetros
set.seed(123)
model_lgb_final <- lgb.train(best_params, dtrain, 1000, valids = list(train = dtrain), early_stopping_rounds = 100, verbose = -1)

# Entrenar otros modelos para el ensamblado
set.seed(123)
model_rf <- randomForest(median_house_value ~ ., data = train_processed[, c(selected_features, "median_house_value")], ntree = 500)
model_xgb <- train(median_house_value ~ ., data = train_processed[, c(selected_features, "median_house_value")], method = "xgbTree", trControl = trainControl(method = "cv", number = 5), tuneLength = 5)

# Predicciones en el conjunto de entrenamiento para ensamblar
train_pred_lgb <- predict(model_lgb_final, as.matrix(train_processed[, selected_features]))
train_pred_rf <- predict(model_rf, train_processed[, selected_features])
train_pred_xgb <- predict(model_xgb, newdata = train_processed[, selected_features])

# Ensamblado de predicciones usando regresión lineal
ensemble_data_train <- data.frame(train_pred_lgb, train_pred_rf, train_pred_xgb)
lm_ensemble <- lm(median_house_value ~ ., data = cbind(ensemble_data_train, median_house_value = train_processed$median_house_value))

# Predicciones en el conjunto de prueba
test_pred_lgb <- predict(model_lgb_final, as.matrix(test_processed[, selected_features]))
test_pred_rf <- predict(model_rf, test_processed[, selected_features])
test_pred_xgb <- predict(model_xgb, newdata = test_processed[, selected_features])

# Ensamblado de predicciones de prueba
ensemble_data_test <- data.frame(test_pred_lgb, test_pred_rf, test_pred_xgb)
test_predictions <- predict(lm_ensemble, newdata = ensemble_data_test)
```

```{r}
# Generar el archivo de salida
submission <- data.frame(id = test_processed$id, median_house_value = test_predictions)
write.csv(submission, "submission_V.csv", row.names = FALSE)
```

