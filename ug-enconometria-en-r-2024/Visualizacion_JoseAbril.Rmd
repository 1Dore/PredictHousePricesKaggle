---
title: "Descartados"
author: "Jose Abril, Diego Alvarez, Oswaldo Lopez"
date: "2024-07-01"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
suppressWarnings(library(tidyverse))
suppressWarnings(library(leaflet))
suppressWarnings(library(cluster))
suppressWarnings(library(factoextra))
suppressWarnings(library(glmnet))
suppressWarnings(library(PerformanceAnalytics))
```

```{r}
# Cargar los datasets
train <- read.csv("train.csv")
test <- read.csv("test.csv")
# Imputar valores nulos en total_bedrooms
train$total_bedrooms[is.na(train$total_bedrooms)] <- median(train$total_bedrooms, na.rm = TRUE)
test$total_bedrooms[is.na(test$total_bedrooms)] <- median(test$total_bedrooms, na.rm = TRUE)
```

# No Funciono / No dio resultado satisfactorios

## Visualizacion de las variables Espaciales
#### Cada intento con clusterizacion no redujo el RMSE y por eso de descarto su uso

### Latitud, Longitud y Ocean Proximity
```{r}
ggplot(train, aes(x = longitude, y = latitude, color = ocean_proximity)) +
  geom_point()  +
  labs(title = "Mapa de precios de casas por coordenadas",
       x = "Longitud",
       y = "Latitud",
       color = "Cercania")
```

### Longitud, Latitud y Median House Value
```{r}
ggplot(train, aes(x = longitude, y = latitude, color = median_house_value)) +
  geom_point() +
  scale_color_gradient(low = "blue", high = "red") +
  labs(title = "Mapa de precios de casas por coordenadas",
       x = "Longitud",
       y = "Latitud",
       color = "Precio de la Casa")
```

### Longitud, Latitud y Median House Value, con mapa
```{r}

m <- leaflet(train) %>%
  addTiles() %>%
  addCircleMarkers(~longitude, ~latitude, radius = ~median_house_value/100000, 
                   color = ~ifelse(median_house_value > median(train$median_house_value), 'red', 'blue'), 
                   popup = ~paste("median_house_value:", median_house_value))

m
```

## Clusterizacion
```{r}
train$ocean_proximity_num <- as.numeric(as.factor(train$ocean_proximity))
```

### Visualizacion Para el Metodo del Codo
```{r}
# Calcular la silueta promedio para k = 2 a 10
k_values <- 2:15

fviz_nbclust(train[, c("longitude", "latitude", "ocean_proximity_num")], kmeans, method = "wss")
```

### Visualizacion Para el Metodo de la Silueta
```{r}
calcular_silueta_promedio <- function(data, k) {
  set.seed(42)
  kmeans_result <- kmeans(data, centers = k)
  silueta <- silhouette(kmeans_result$cluster, dist(data))
  mean(silueta[, 3])  # Promedio del coeficiente de silueta
}

silueta_promedio <- sapply(k_values, function(k) calcular_silueta_promedio(train[, c("longitude", "latitude", "ocean_proximity_num")], k))


# Visualizar los resultados
plot(k_values, silueta_promedio, type = "b", pch = 19,
     xlab = "Número de Clusters (k)", ylab = "Coeficiente de Silueta Promedio",
     main = "Coeficiente de Silueta Promedio para Diferentes k")

```

### Visualizacion de los clusters y su posicion
```{r}
num_clusters <- 5


clusters <- kmeans(train[, c("longitude", "latitude", "ocean_proximity_num")], centers = num_clusters)
train$position_cluster <- as.factor(clusters$cluster)

ggplot(train, aes(x = longitude, y = latitude, color = position_cluster)) +
  geom_point() +
  labs(title = "Clustering de casas por coordenadas",
       x = "Longitud",
       y = "Latitud",
       color = "Cluster")
```

## Eliminar Outliers
#### Se descarto eliminar outliers porque provocaba overfiting

```{r}
continuas =  names(train)[sapply(train, is.numeric) & names(train) != "median_house_value" & names(train) != "id"  &   30 < sapply(train, n_distinct)  ]
continuas
```


### Pre Eliminacion
```{r fig.height=8, fig.width=9}

for (column_name  in  continuas) {
  
  vector_numerico = unlist(train[column_name])
  
  Q1 <- quantile(vector_numerico, 0.25)  # Primer cuartil (Q1)
  Q3 <- quantile(vector_numerico, 0.75)  # Tercer cuartil (Q3)
  IQR <- Q3 - Q1  # Rango intercuartílico
  
  # Definir los límites inferior y superior para identificar outliers
  limite_inferior <- Q1 - 1.5 * IQR
  limite_superior <- Q3 + 1.5 * IQR
  
  # Identificar los outliers
  outliers <- vector_numerico[vector_numerico < limite_inferior | vector_numerico > limite_superior]
  
  # Contar los outliers
  conteo_outliers <- length(outliers)
  
  # Resultados
  print(column_name)
  print(paste("Número de outliers identificados usando IQR:", conteo_outliers))
  print(paste("Prosnetaje de outliers identificados usando IQR:", conteo_outliers/nrow(train)))
}

suppressWarnings(chart.Correlation(train[continuas], histogram=TRUE))

```

## Eliminacion 
```{r}
remove_outliers <- function(df, columns) {
  for (col in columns) {
    Q1 <- quantile(df[[col]], 0.25)
    Q3 <- quantile(df[[col]], 0.75)
    IQR <- Q3 - Q1
    df <- df[!(df[[col]] < (Q1 - 1.5 * IQR) | df[[col]] > (Q3 + 1.5 * IQR)), ]
  }
  return(df)
}

train_no_outliers <- remove_outliers(train, continuas)

cat("Tamaño del dataset train_processed original: ", nrow(train), "\n")
cat("Tamaño del dataset train_processed sin outliers: ", nrow(train_no_outliers), "\n")
```

### Post eliminacion
```{r fig.height=8, fig.width=9}
for (column_name  in  continuas) {
  
  vector_numerico = unlist(train_no_outliers[column_name])
  
  Q1 <- quantile(vector_numerico, 0.25)  # Primer cuartil (Q1)
  Q3 <- quantile(vector_numerico, 0.75)  # Tercer cuartil (Q3)
  IQR <- Q3 - Q1  # Rango intercuartílico
  
  # Definir los límites inferior y superior para identificar outliers
  limite_inferior <- Q1 - 1.5 * IQR
  limite_superior <- Q3 + 1.5 * IQR
  
  # Identificar los outliers
  outliers <- vector_numerico[vector_numerico < limite_inferior | vector_numerico > limite_superior]
  
  # Contar los outliers
  conteo_outliers <- length(outliers)
  
  # Resultados
  print(column_name)
  print(paste("Número de outliers identificados usando IQR:", conteo_outliers))
  print(paste("Prosnetaje de outliers identificados usando IQR:", conteo_outliers/nrow(train)))
}

suppressWarnings(chart.Correlation(train_no_outliers[continuas], histogram=TRUE))
```

## Modelos Descartados
- Lineal
- Multilineal
- Ridgge
- Lasso
- Elastic
- Random Forest

## Usados, pero superados
- Extream Gradiant Boostiong

